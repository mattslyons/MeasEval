{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, Adam\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import RobertaModel\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "reprocess_raw =  False\n",
    "\n",
    "batch_size = 8 # documents\n",
    "learning_rate = 5e-5\n",
    "n_epochs = 10\n",
    "\n",
    "# task_map = {'Quantity':1}\n",
    "task_map = {'Quantity':1,'MeasuredProperty':2,'MeasuredEntity':3,'Qualifier':4} # uncomment for multi-class\n",
    "num_classes = len(task_map)\n",
    "\n",
    "model_name = 'roberta-base'\n",
    "# model_name = 'bert-base-cased'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = 'cpu' # uncomment this to make debugging easier\n",
    "\n",
    "data_size_reduce = 1 # multiplier for making small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir = os.getcwd() # ~/MeasEval/baselines\n",
    "\n",
    "combopath_txt = os.path.join(currentdir, \"../data/raw/combo/text/\")\n",
    "combopath_annot = os.path.join(currentdir, \"../data/raw/combo/tsv/\")\n",
    "\n",
    "interimpath = os.path.join(currentdir, \"../data/interim/\")\n",
    "\n",
    "outputpath = os.path.join(currentdir, \"../data/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_txt(docs):\n",
    "    processesd_txt = {}\n",
    "    remove_markers = True\n",
    "\n",
    "    cnt_toks = {\"figs.\": 0, \"fig.\": 0, \"et al.\": 0,\n",
    "            \"ref.\": 0, \"eq.\": 0, \"e.g.\": 0,\n",
    "            \"i.e.\": 0, \"nos.\": 0, \"no.\": 0,\n",
    "            \"spp.\": 0\n",
    "            }\n",
    "    regex_end_checker = [\".*[a-zA-Z]figs\\.$\", \n",
    "                        \".*[a-zA-Z]fig\\.$\",\n",
    "                        \".*[a-zA-Z]et al\\.$\",\n",
    "                        \".*[a-zA-Z]ref\\.$\",\n",
    "                        \".*[a-zA-Z]eq\\.$\",\n",
    "                        \".*[a-zA-Z]e\\.g\\.$\",\n",
    "                        \".*[a-zA-Z]i\\.e\\.$\",\n",
    "                        \".*[a-zA-Z]nos\\.$\",\n",
    "                        \".*[a-zA-Z]no\\.$\",\n",
    "                        \".*[a-zA-Z]spp\\.$\",\n",
    "                        # figs., fig., et al., Ref., Eq., e.g., i.e., Nos., No., spp.\n",
    "                    ]\n",
    "\n",
    "    assert len(cnt_toks) == len(regex_end_checker)\n",
    "\n",
    "    for docId, doc in docs.items():\n",
    "        flag = False\n",
    "        sentences = sent_tokenize(doc)\n",
    "\n",
    "        fixed_sentence_tokens = []\n",
    "        curr_len = 0\n",
    "        for s in sentences:\n",
    "            if flag == True:\n",
    "                assert s[0] != ' '\n",
    "                white_length = doc[curr_len:].find(s[0])\n",
    "\n",
    "                prev_len = len(fixed_sentence_tokens[-1])\n",
    "                fixed_sentence_tokens[-1] = fixed_sentence_tokens[-1] + (\" \"*white_length) + s\n",
    "\n",
    "                assert fixed_sentence_tokens[-1][prev_len+white_length] == doc[curr_len+white_length], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = white_length + len(s)\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "            else:\n",
    "                if len(fixed_sentence_tokens) != 0:\n",
    "                    assert s[0] != ' '\n",
    "                    white_length = doc[curr_len:].find(s[0])\n",
    "                    fixed_sentence_tokens.append( (\" \"*white_length) + s )\n",
    "                else:\n",
    "                    fixed_sentence_tokens.append(s)\n",
    "                assert fixed_sentence_tokens[-1][0] == doc[curr_len], (fixed_sentence_tokens, doc, curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = len(fixed_sentence_tokens[-1])\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "\n",
    "            lower_cased_s = fixed_sentence_tokens[-1].lower()\n",
    "            flag = False\n",
    "            if remove_markers:\n",
    "                for i, k in enumerate(cnt_toks):\n",
    "                    this_regex_pattern = regex_end_checker[i]\n",
    "                    if lower_cased_s.endswith(k) and re.match(this_regex_pattern, lower_cased_s) == None:\n",
    "                        cnt_toks[k] += 1\n",
    "                        flag = True\n",
    "                        break\n",
    "\n",
    "        processesd_txt[docId] = ''.join(fixed_sentence_tokens)\n",
    "    return processesd_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(reprocess_raw = False):\n",
    "\n",
    "    if reprocess_raw == True:\n",
    "        docIds = []\n",
    "        combo_txt = {}\n",
    "        for fn in os.listdir(combopath_txt):\n",
    "            docIds.append(fn[:-4])\n",
    "            path = combopath_txt+fn\n",
    "            with open(path) as textfile:\n",
    "                    text = textfile.read()\n",
    "                    #[:-4] strips off the .txt to get the id\n",
    "                    combo_txt[fn[:-4]] = text\n",
    "\n",
    "        combo_annot = pd.DataFrame()\n",
    "        for fn in os.listdir(combopath_annot):\n",
    "            path = combopath_annot+fn\n",
    "            file = pd.read_csv(path,delimiter='\\t',encoding='utf-8')\n",
    "            combo_annot = pd.concat([combo_annot, file],ignore_index=True)\n",
    "\n",
    "        combo_txt = process_raw_txt(combo_txt)\n",
    "        assert docIds == list(combo_txt.keys()), (len(docIds), len(list(combo_txt.keys())))\n",
    "\n",
    "        with open(interimpath+'combo_txt.json','w') as f:\n",
    "            json.dump(combo_txt, f)\n",
    "\n",
    "        combo_annot.to_csv(interimpath+'combo_annot.csv')\n",
    "\n",
    "        return docIds, combo_txt, combo_annot\n",
    "    else:\n",
    "        combo_annot = pd.read_csv(interimpath+'combo_annot.csv')\n",
    "\n",
    "        with open(interimpath+'combo_txt.json','r') as f:\n",
    "            combo_txt = json.load(f)\n",
    "\n",
    "        docIds = list(combo_txt.keys())\n",
    "    \n",
    "        return docIds, combo_txt, combo_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_docs, combo_txt, combo_annot = read_data(reprocess_raw = reprocess_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train/dev/test split options\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "percent_to_test = .1\n",
    "percent_to_dev = .2\n",
    "percent_to_train =  1 - percent_to_dev - percent_to_test\n",
    "\n",
    "n_doc = len(combo_docs)\n",
    "split_train = int(np.round(n_doc * percent_to_train))\n",
    "split_dev = split_train + int(np.round(n_doc * percent_to_dev))\n",
    "\n",
    "train_docs = combo_docs[:split_train]\n",
    "dev_docs = combo_docs[split_train:split_dev]\n",
    "test_docs = combo_docs[split_dev:]\n",
    "\n",
    "train_docs = random.sample(train_docs, int(len(train_docs)*data_size_reduce))\n",
    "dev_docs = random.sample(dev_docs, int(len(dev_docs)*data_size_reduce))\n",
    "test_docs = random.sample(test_docs, int(len(test_docs)*data_size_reduce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Tokenizer ###########\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>annotId</th>\n",
       "      <th>annotSet</th>\n",
       "      <th>annotType</th>\n",
       "      <th>annotSpan</th>\n",
       "      <th>subSpanType</th>\n",
       "      <th>linkId</th>\n",
       "      <th>linkSpan</th>\n",
       "      <th>subSpan</th>\n",
       "      <th>unit</th>\n",
       "      <th>unitEncoded</th>\n",
       "      <th>misc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comboId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S0167577X14001256-517_T1-1</th>\n",
       "      <td>S0167577X14001256-517</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[355, 365]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nJ</td>\n",
       "      <td>[282, 863]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0167880913001229-1304_T4-3</th>\n",
       "      <td>S0167880913001229-1304</td>\n",
       "      <td>T4-3</td>\n",
       "      <td>3</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[447, 452]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-3</td>\n",
       "      <td>[439, 446]</td>\n",
       "      <td>[439, 452]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0378112713005288-1800_T4-6</th>\n",
       "      <td>S0378112713005288-1800</td>\n",
       "      <td>T4-6</td>\n",
       "      <td>6</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[971, 989]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-6</td>\n",
       "      <td>[953, 965]</td>\n",
       "      <td>[953, 989]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0019103512002801-1342_T23-3</th>\n",
       "      <td>S0019103512002801-1342</td>\n",
       "      <td>T23-3</td>\n",
       "      <td>3</td>\n",
       "      <td>Qualifier</td>\n",
       "      <td>[162, 179]</td>\n",
       "      <td>Qualifies</td>\n",
       "      <td>T43-3</td>\n",
       "      <td>[133, 145]</td>\n",
       "      <td>[133, 179]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0167739X12001525-6016_T3-3</th>\n",
       "      <td>S0167739X12001525-6016</td>\n",
       "      <td>T3-3</td>\n",
       "      <td>3</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[501, 525]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-3</td>\n",
       "      <td>[473, 490]</td>\n",
       "      <td>[473, 525]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0012821X12004384-1284_T104-10</th>\n",
       "      <td>S0012821X12004384-1284</td>\n",
       "      <td>T104-10</td>\n",
       "      <td>10</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[1689, 1707]</td>\n",
       "      <td>HasProperty</td>\n",
       "      <td>T94-10</td>\n",
       "      <td>[1658, 1681]</td>\n",
       "      <td>[1658, 1707]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0168945213001805-4775_T2-4</th>\n",
       "      <td>S0168945213001805-4775</td>\n",
       "      <td>T2-4</td>\n",
       "      <td>4</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[885, 901]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-4</td>\n",
       "      <td>[905, 910]</td>\n",
       "      <td>[885, 910]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 docId  annotId  annotSet  \\\n",
       "comboId                                                                     \n",
       "S0167577X14001256-517_T1-1       S0167577X14001256-517     T1-1         1   \n",
       "S0167880913001229-1304_T4-3     S0167880913001229-1304     T4-3         3   \n",
       "S0378112713005288-1800_T4-6     S0378112713005288-1800     T4-6         6   \n",
       "S0019103512002801-1342_T23-3    S0019103512002801-1342    T23-3         3   \n",
       "S0167739X12001525-6016_T3-3     S0167739X12001525-6016     T3-3         3   \n",
       "S0012821X12004384-1284_T104-10  S0012821X12004384-1284  T104-10        10   \n",
       "S0168945213001805-4775_T2-4     S0168945213001805-4775     T2-4         4   \n",
       "\n",
       "                                       annotType     annotSpan  subSpanType  \\\n",
       "comboId                                                                       \n",
       "S0167577X14001256-517_T1-1              Quantity    [355, 365]          NaN   \n",
       "S0167880913001229-1304_T4-3     MeasuredProperty    [447, 452]  HasQuantity   \n",
       "S0378112713005288-1800_T4-6     MeasuredProperty    [971, 989]  HasQuantity   \n",
       "S0019103512002801-1342_T23-3           Qualifier    [162, 179]    Qualifies   \n",
       "S0167739X12001525-6016_T3-3     MeasuredProperty    [501, 525]  HasQuantity   \n",
       "S0012821X12004384-1284_T104-10    MeasuredEntity  [1689, 1707]  HasProperty   \n",
       "S0168945213001805-4775_T2-4     MeasuredProperty    [885, 901]  HasQuantity   \n",
       "\n",
       "                                linkId      linkSpan       subSpan unit  \\\n",
       "comboId                                                                   \n",
       "S0167577X14001256-517_T1-1         NaN           NaN           NaN   nJ   \n",
       "S0167880913001229-1304_T4-3       T1-3    [439, 446]    [439, 452]  NaN   \n",
       "S0378112713005288-1800_T4-6       T1-6    [953, 965]    [953, 989]  NaN   \n",
       "S0019103512002801-1342_T23-3     T43-3    [133, 145]    [133, 179]  NaN   \n",
       "S0167739X12001525-6016_T3-3       T1-3    [473, 490]    [473, 525]  NaN   \n",
       "S0012821X12004384-1284_T104-10  T94-10  [1658, 1681]  [1658, 1707]  NaN   \n",
       "S0168945213001805-4775_T2-4       T1-4    [905, 910]    [885, 910]  NaN   \n",
       "\n",
       "                               unitEncoded misc  \n",
       "comboId                                          \n",
       "S0167577X14001256-517_T1-1      [282, 863]  NaN  \n",
       "S0167880913001229-1304_T4-3            NaN  NaN  \n",
       "S0378112713005288-1800_T4-6            NaN  NaN  \n",
       "S0019103512002801-1342_T23-3           NaN  NaN  \n",
       "S0167739X12001525-6016_T3-3            NaN  NaN  \n",
       "S0012821X12004384-1284_T104-10         NaN  NaN  \n",
       "S0168945213001805-4775_T2-4            NaN  NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_annotation_set(annot_set):\n",
    "\n",
    "    annot_set_processed = []\n",
    "\n",
    "    annot_set['comboIds'] = annot_set[['docId','annotId']].agg('_'.join, axis=1)\n",
    "    annot_set.set_index('comboIds',inplace=True)\n",
    "\n",
    "    for comboId in list(annot_set.index):\n",
    "        \n",
    "        docId = annot_set.loc[comboId]['docId']\n",
    "        annotId = annot_set.loc[comboId]['annotId']\n",
    "        annotSet = annot_set.loc[comboId]['annotSet']\n",
    "        annotType = annot_set.loc[comboId]['annotType']\n",
    "        annotSpan = [annot_set.loc[comboId]['startOffset'],annot_set.loc[comboId]['endOffset']]\n",
    "\n",
    "        ent_annot_processed = {\n",
    "            'comboId':comboId,\n",
    "            'docId':docId,\n",
    "            'annotId':annotId,\n",
    "            'annotSet':annotSet,\n",
    "            'annotType':annotType,\n",
    "            'annotSpan':annotSpan,\n",
    "            'subSpanType':np.nan,\n",
    "            'linkId':np.nan,\n",
    "            'linkSpan':np.nan,\n",
    "            'subSpan':np.nan,\n",
    "            'unit':np.nan,\n",
    "            'unitEncoded':np.nan,\n",
    "            'misc':np.nan\n",
    "        }\n",
    "        \n",
    "        other = annot_set.loc[comboId]['other']\n",
    "        if isinstance(other,str):\n",
    "            otherDict = json.loads(str(other))\n",
    "\n",
    "            if annot_set.loc[comboId]['annotType'] != 'Quantity':\n",
    "\n",
    "                ent_annot_processed['subSpanType'] = list(otherDict.keys())[0]\n",
    "                link = list(otherDict.values())[0]\n",
    "\n",
    "                ent_annot_processed['linkId'] = link\n",
    "                linkIdx = docId+'_'+link\n",
    "                linkSpan = [int(annot_set.loc[linkIdx]['startOffset']),int(annot_set.loc[linkIdx]['endOffset'])]\n",
    "                ent_annot_processed['linkSpan'] = linkSpan\n",
    "\n",
    "                spanEnds = annotSpan + linkSpan\n",
    "                ent_annot_processed['subSpan'] = [min(spanEnds),max(spanEnds)]\n",
    "\n",
    "            elif 'unit' in list(otherDict.keys()):\n",
    "                unit = otherDict['unit']\n",
    "                ent_annot_processed['unit'] = unit\n",
    "                ent_annot_processed['unitEncoded'] = tokenizer.encode(unit)[1:-1]\n",
    "            else:\n",
    "                ent_annot_processed['misc'] = otherDict\n",
    "\n",
    "\n",
    "        annot_set_processed.append(ent_annot_processed)\n",
    "   \n",
    "    return pd.DataFrame.from_dict(annot_set_processed).set_index('comboId')\n",
    "\n",
    "combo_annot_processed = process_annotation_set(combo_annot)\n",
    "combo_annot_processed.to_csv(interimpath+'combo_annot_processed.csv')\n",
    "combo_annot_processed.sample(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert special tokens for subspans (Sam)\n",
    "# will make docs longer\n",
    "\n",
    "# def char_map(doc_annot, task_map)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(\n",
    "                                doc_list=combo_docs,\n",
    "                                txt=combo_txt,\n",
    "                                processed_annotation=combo_annot_processed,\n",
    "                                tokenizer=tokenizer,\n",
    "                                taskLabelMap=task_map\n",
    "                            ):\n",
    "\n",
    "    toks_with_labels = []\n",
    "    special_ids = tokenizer.all_special_ids\n",
    "    collisionDict = {}\n",
    "\n",
    "    for doc in doc_list:\n",
    "        within_doc_collision_count = 0\n",
    "        # print(doc)\n",
    "        # print(processed_annotation.loc[processed_annotation['docId'] == doc])\n",
    "        doc_annot = processed_annotation.loc[processed_annotation['docId'] == doc]\n",
    "        doc_annot.set_index('annotId',inplace=True)\n",
    "        # print(doc_annot)\n",
    "\n",
    "        encoded_txt = tokenizer(txt[doc], padding='max_length', max_length=512, truncation=True)\n",
    "        encoded_tokens = encoded_txt['input_ids']\n",
    "        # print(encoded_tokens)\n",
    "\n",
    "        ############### Label Primary Spans ###############\n",
    "\n",
    "        labelIds = np.full(len(encoded_tokens),-1)\n",
    "        taskCharMap = {} # \n",
    "        taskCharList = []\n",
    "        collision = False\n",
    "\n",
    "        if len(doc_annot) > 0:\n",
    "            for annot_idx in range(int(doc_annot['annotSet'].max())):\n",
    "                # print(annot_idx)\n",
    "                if collision:\n",
    "                    taskCharMap = taskCharMapBackup\n",
    "                taskCharMapBackup = copy.deepcopy(taskCharMap)\n",
    "                collision = False\n",
    "                spans = list(doc_annot.loc[doc_annot['annotSet']==annot_idx+1]['annotSpan'])\n",
    "                annotset_tasks = list(doc_annot.loc[doc_annot['annotSet']==annot_idx+1]['annotType'])\n",
    "                for i in range(len(spans)):\n",
    "                    span = spans[i]\n",
    "                    annotset_task = annotset_tasks[i]\n",
    "                    # print(span)\n",
    "                    # print(annotset_task)\n",
    "                    if annotset_task in taskLabelMap.keys():\n",
    "                        for spanCharIdx in span:\n",
    "                            if spanCharIdx in taskCharMap:\n",
    "                                if taskCharMap[spanCharIdx] != taskLabelMap[annotset_task]:\n",
    "                                    # print(\"=\"*45)\n",
    "                                    # print(\"Collision detected in doc\",doc)\n",
    "                                    # print(\"Previous mapped task:\",taskCharMap[spanCharIdx],\"new mapped task:\",taskLabelMap[annotset_task])\n",
    "                                    # print(\"Current span:\",span)\n",
    "                                    # print(\"Second (offending) annotSet will not be included.\")\n",
    "                                    # print(\"=\"*45)\n",
    "                                    collisionDict[doc + '_' + \n",
    "                                                  str(within_doc_collision_count \n",
    "                                                      + 1)] = span\n",
    "                                    collision = True\n",
    "                                    within_doc_collision_count += 1\n",
    "                                    break\n",
    "                            # print(spanCharIdx)\n",
    "                            taskCharMap[spanCharIdx] = taskLabelMap[annotset_task]\n",
    "                        if collision:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "    \n",
    "                        # print(taskCharMap)\n",
    "                            # taskAnnotIdCharMap[spanCharIdx] = annotId\n",
    "            \n",
    "\n",
    "        decoded = [''] * len(encoded_tokens)\n",
    "        for tokenIdx, token in enumerate(encoded_tokens):\n",
    "            \n",
    "            if token not in special_ids:\n",
    "                tokenCharStart = encoded_txt.token_to_chars(tokenIdx).start\n",
    "                if tokenCharStart in list(taskCharMap.keys()):\n",
    "                    labelIds[tokenIdx] = taskCharMap[tokenCharStart]\n",
    "                    decoded[tokenIdx] = tokenizer.decode(token)\n",
    "                else:\n",
    "                    labelIds[tokenIdx] = 0\n",
    "            else:\n",
    "                labelIds[tokenIdx] = 0\n",
    "        \n",
    "\n",
    "        ############### Sub Spans Token Insertion and labeling ###############\n",
    "\n",
    "        encoded_txt['doc_or_sent_id'] = doc\n",
    "        encoded_txt['labels'] = labelIds\n",
    "        \n",
    "        toks_with_labels.append(encoded_txt)\n",
    "        \n",
    "    print(\"Total collisions avoided:\", len(collisionDict))\n",
    "    \n",
    "    # return toks_with_labels\n",
    "    return pd.DataFrame.from_dict(toks_with_labels), collisionDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total collisions avoided: 136\n",
      "Total collisions avoided: 33\n",
      "Total collisions avoided: 4\n"
     ]
    }
   ],
   "source": [
    "################# TOKENIZE #################\n",
    "\n",
    "stage1_train_ds, train_collisions = tokenize_and_align_labels(\n",
    "    doc_list=train_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_train_ds.to_csv(interimpath+'stage1_train_ds.csv')\n",
    "stage1_n_train = stage1_train_ds.shape[0]\n",
    "\n",
    "\n",
    "stage1_dev_ds, dev_collisions = tokenize_and_align_labels(\n",
    "    doc_list=dev_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_dev_ds.to_csv(interimpath+'stage1_dev_ds.csv')\n",
    "stage1_n_dev = stage1_dev_ds.shape[0]\n",
    "\n",
    "stage1_test_ds, test_collisions = tokenize_and_align_labels(\n",
    "    doc_list=test_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_test_ds.to_csv(interimpath+'stage1_test_ds.csv')\n",
    "stage1_n_test = stage1_test_ds.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matt\n",
    "# def shorten_txt_encoding(txt, shorten_by : int):       \n",
    "#     pass...\n",
    "\n",
    "# generate a list of docIds that have token collision after shortening\n",
    "\n",
    "# toks = list(stage1_dev_ds.sample(1)['input_ids'])\n",
    "\n",
    "# print(toks[0])\n",
    "\n",
    "# tokenizer.decode(toks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(tokenized_dataset, batch_size, device):\n",
    "    num_examples = int(tokenized_dataset.shape[0] / batch_size)\n",
    "    batch_sizes = [batch_size for x in range(num_examples)]\n",
    "    last_batch_size = tokenized_dataset.shape[0] % batch_size\n",
    "    if last_batch_size:\n",
    "        batch_sizes.append(last_batch_size)\n",
    "    # print(batch_sizes)\n",
    "\n",
    "    batched_dataset = []\n",
    "\n",
    "    idf_to_torch = lambda df : torch.tensor(np.array([list(map(int,r)) for r in df])).to(device)\n",
    "\n",
    "    for idx, size in enumerate(batch_sizes):\n",
    "        start = sum(batch_sizes[:idx])\n",
    "        end = sum(batch_sizes[:idx]) + size - 1\n",
    "        # print(start,end,idx)\n",
    "        input_ids = idf_to_torch(tokenized_dataset['input_ids'].loc[start:end])\n",
    "        attention_mask = idf_to_torch(tokenized_dataset['attention_mask'].loc[start:end])\n",
    "        labels = idf_to_torch(tokenized_dataset['labels'].loc[start:end])\n",
    "        doc_or_sent_id = list(tokenized_dataset['doc_or_sent_id'].loc[start:end])\n",
    "        \n",
    "        batch = {\n",
    "            'input_ids':input_ids,\n",
    "            'labels':labels,\n",
    "            'attention_mask':attention_mask,\n",
    "            'doc_or_sent_id':doc_or_sent_id\n",
    "\n",
    "        }\n",
    "        \n",
    "        batched_dataset.append(batch)\n",
    "\n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# batchify ####################\n",
    "\n",
    "batched_train_ds = batchify(stage1_train_ds[['attention_mask','input_ids','labels','doc_or_sent_id']], batch_size, device)\n",
    "batched_dev_ds = batchify(stage1_dev_ds[['attention_mask','input_ids','labels','doc_or_sent_id']], batch_size, device)\n",
    "batched_test_ds = batchify(stage1_test_ds[['attention_mask','input_ids','labels','doc_or_sent_id']], batch_size, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 44105,     4,  ...,     1,     1,     1],\n",
       "         [    0,   970,    32,  ...,     1,     1,     1],\n",
       "         [    0,   170,   220,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,   500, 26343,  ...,     1,     1,     1],\n",
       "         [    0, 44105,     4,  ...,     1,     1,     1],\n",
       "         [    0, 14699,    12,  ...,     1,     1,     1]], device='cuda:0'),\n",
       " 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 3, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 3, 0,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'doc_or_sent_id': ['S0016236113008041-3290',\n",
       "  'S0006322312001096-1271',\n",
       "  'S0022000014000026-7850',\n",
       "  'S0019103513005058-4158',\n",
       "  'S0019103512004009-4492',\n",
       "  'S0019103512002801-1342',\n",
       "  'S0016236113008041-3171',\n",
       "  'S0378383912000130-1096']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Quantity': 1, 'MeasuredProperty': 2, 'MeasuredEntity': 3, 'Qualifier': 4}\n"
     ]
    }
   ],
   "source": [
    "demo_batch = 2\n",
    "\n",
    "demo_batch = batched_train_ds[demo_batch]\n",
    "\n",
    "\n",
    "demo_doc = demo_batch['doc_or_sent_id'][0]\n",
    "demo_ids = demo_batch['input_ids'].cpu().numpy()[0]\n",
    "demo_tokens = tokenizer.decode(demo_batch['input_ids'].cpu().numpy()[0])\n",
    "demo_labels = demo_batch['labels'].cpu().numpy()[0]\n",
    "demo_mask = demo_batch['attention_mask'].cpu().numpy()[0]\n",
    "latch_print = False\n",
    "labeled_tokens = ''\n",
    "for id, lab in zip(demo_ids, demo_labels):\n",
    "    if lab:\n",
    "        labeled_tokens = labeled_tokens + tokenizer.decode(id) + ' '\n",
    "\n",
    "print(task_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S0301010413004096-693'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0 44791  2383 34098  7208    36  8756   597    43 34003    32    10\n",
      "  2849    12  4684     9 35443  9281  7823    61   311   372  4198    13\n",
      "  1123  3521     8 10875   528     7    49   239  4084   443     6   614\n",
      "  7208 16522     6     8  8859   868 12628   181  1688  1737   646   246\n",
      "  8174 11757   597  3183    32  2333  1490    62    31  4204 41985    50\n",
      " 28255 30987  4462    30  6523  3104   268     7  4960   155   495  3112\n",
      " 32480    19     5  9285     9 32426  2192  6272    31 14926  6884 24477\n",
      "     7 10969  1517 24477   976     4  3646   453   624    42 11757   597\n",
      "   284    33  4824 13113  6608   239   289   176  5814   368 24802 23549\n",
      "    36 41324    23  8930 23982  3971     6  3700    23  6791   229    43\n",
      "   646   306   742    19    10   638     9 49447  1549   885    90   207\n",
      "   746 33646  2148  6373    11   234   791    12  1866   646   245   742\n",
      "     8 11757   597    12  2619   646   401  8174   635     6   209   239\n",
      " 33646 23549  1874  8617    19  2284  5181     6     8  4634  4146    16\n",
      "    10  7708  1468     4   345    16  4634  1989  9723    15 17775  3009\n",
      "     5 11324   227 11757   597  4452     8  5814   368  5134   289   176\n",
      " 20237     6     8     5 10614     9  2167 17014 11324     8  3611     9\n",
      " 20038   624 18687   980  3372    41   505 18670    13     5   709     9\n",
      "   357  3183    14   189   483   201     7  1743     9  7708   304     4\n",
      "    96 20379 41987 10477 25871 22870    36   487  6153    43    23   874\n",
      "   158   229    34    57   341  1433     7  3094     5  3237     9   211\n",
      "   176   624    10   367   275    12 24170 13286 11757   597  3183 22690\n",
      "  4924  4204  3091   646   406  2383  1092  8174    85    34    57   303\n",
      "    14   211   176    64 23379  2024     7 11042  3091    15  4204  7872\n",
      "     6     8    14     5  5814   368  5134   211   176 20237    33 22481\n",
      " 31207  1635 10451     7    14     7   211   176    11     5  2705   194\n",
      "     4  1216  3218    33  1286 21991  9825 23437    13    49  6373   239\n",
      "  1123  5814   368 24802 23549     4  1624    34  4634  2061 26014    15\n",
      " 11757 34417    19   239   289   176 33646 23549     6   150  3183  2018\n",
      "   182   614   289   176 33646     8    73   368 14518  1950 13662  4204\n",
      "  7872    32   747  8266    13    42   892     4  9068     6   335    15\n",
      " 17014 11324   624   167   614    12 29809  5113 11757   597  1743    16\n",
      "  4378 12622     6    53    64   202   492   505 25402   414     8   801\n",
      "  2969    13     5  7757  1521     8 17775  3258     9 18303  3521  3183\n",
      "     4     2     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n"
     ]
    }
   ],
   "source": [
    "print(demo_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0\n",
      " 3 0 2 2 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(demo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Metal–organic framework (MOF) complexes are a sub-class of porous solids which show great promise for gas storage and separation due to their high surface area, low framework density, and tuneable functional pore environment [3]. MOF materials are usually built up from metal ions or clusters bridged by organic linkers to afford 3D extended frameworks with the formation of cavities ranging from microporous to mesoporous region. Several members within this MOF family have achieved impressively high H2 adsorption capacities (albeit at cryogenic temperatures, typically at 77 K) [4] with a record of ∼16 wt% total uptake capacity observed in NU-100 [5] and MOF-200 [6]. However, these high uptake capacities drop dramatically with increasing temperature, and thus none is a practical material. There is thus particular emphasis on optimising the interactions between MOF hosts and adsorbed H2 molecules, and the identification of specific binding interactions and properties of gases within confined space represents an important methodology for the development of better materials that may lead us to systems of practical use. In situ neutron powder diffraction (NPD) at below 10 K has been used previously to determine the locations of D2 within a few best-behaving MOF materials incorporating exposed metal sites [7–12]. It has been found that D2 can bind directly to vacant sites on metal centres, and that the adsorbed D2 molecules have molecular separations comparable to that to D2 in the solid state. These studies have provided invaluable structural rationale for their observed high gas adsorption capacities. Research has thus focused understandably on MOFs with high H2 uptake capacities, while materials showing very low H2 uptake and/or incorporate fully coordinated metal centres are often ignored for this study. Therefore, information on binding interactions within those low-uptake MOF systems is entirely lacking, but can still give important complementary data and potential understanding for the subsequent design and optimisation of hydrogen storage materials.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(demo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>annotId</th>\n",
       "      <th>annotSet</th>\n",
       "      <th>annotType</th>\n",
       "      <th>annotSpan</th>\n",
       "      <th>subSpanType</th>\n",
       "      <th>linkId</th>\n",
       "      <th>linkSpan</th>\n",
       "      <th>subSpan</th>\n",
       "      <th>unit</th>\n",
       "      <th>unitEncoded</th>\n",
       "      <th>misc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comboId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S0301010413004096-693_T1-1</th>\n",
       "      <td>S0301010413004096-693</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[575, 579]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K</td>\n",
       "      <td>[530]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0301010413004096-693_T3-1</th>\n",
       "      <td>S0301010413004096-693</td>\n",
       "      <td>T3-1</td>\n",
       "      <td>1</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[548, 560]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>[575, 579]</td>\n",
       "      <td>[548, 579]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0301010413004096-693_T4-1</th>\n",
       "      <td>S0301010413004096-693</td>\n",
       "      <td>T4-1</td>\n",
       "      <td>1</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[538, 547]</td>\n",
       "      <td>HasProperty</td>\n",
       "      <td>T3-1</td>\n",
       "      <td>[548, 560]</td>\n",
       "      <td>[538, 560]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0301010413004096-693_T1-2</th>\n",
       "      <td>S0301010413004096-693</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>2</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[602, 609]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wt%</td>\n",
       "      <td>[43192, 207]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0301010413004096-693_T3-2</th>\n",
       "      <td>S0301010413004096-693</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>2</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[610, 631]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>[602, 609]</td>\n",
       "      <td>[602, 631]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0301010413004096-693_T4-2</th>\n",
       "      <td>S0301010413004096-693</td>\n",
       "      <td>T4-2</td>\n",
       "      <td>2</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[502, 504]</td>\n",
       "      <td>HasProperty</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>[610, 631]</td>\n",
       "      <td>[502, 631]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0301010413004096-693_T5-2</th>\n",
       "      <td>S0301010413004096-693</td>\n",
       "      <td>T5-2</td>\n",
       "      <td>2</td>\n",
       "      <td>Qualifier</td>\n",
       "      <td>[431, 469]</td>\n",
       "      <td>Qualifies</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>[610, 631]</td>\n",
       "      <td>[431, 631]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0301010413004096-693_T1-3</th>\n",
       "      <td>S0301010413004096-693</td>\n",
       "      <td>T1-3</td>\n",
       "      <td>3</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[1174, 1184]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K</td>\n",
       "      <td>[530]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0301010413004096-693_T3-3</th>\n",
       "      <td>S0301010413004096-693</td>\n",
       "      <td>T3-3</td>\n",
       "      <td>3</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[1130, 1170]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-3</td>\n",
       "      <td>[1174, 1184]</td>\n",
       "      <td>[1130, 1184]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            docId annotId  annotSet  \\\n",
       "comboId                                                               \n",
       "S0301010413004096-693_T1-1  S0301010413004096-693    T1-1         1   \n",
       "S0301010413004096-693_T3-1  S0301010413004096-693    T3-1         1   \n",
       "S0301010413004096-693_T4-1  S0301010413004096-693    T4-1         1   \n",
       "S0301010413004096-693_T1-2  S0301010413004096-693    T1-2         2   \n",
       "S0301010413004096-693_T3-2  S0301010413004096-693    T3-2         2   \n",
       "S0301010413004096-693_T4-2  S0301010413004096-693    T4-2         2   \n",
       "S0301010413004096-693_T5-2  S0301010413004096-693    T5-2         2   \n",
       "S0301010413004096-693_T1-3  S0301010413004096-693    T1-3         3   \n",
       "S0301010413004096-693_T3-3  S0301010413004096-693    T3-3         3   \n",
       "\n",
       "                                   annotType     annotSpan  subSpanType  \\\n",
       "comboId                                                                   \n",
       "S0301010413004096-693_T1-1          Quantity    [575, 579]          NaN   \n",
       "S0301010413004096-693_T3-1  MeasuredProperty    [548, 560]  HasQuantity   \n",
       "S0301010413004096-693_T4-1    MeasuredEntity    [538, 547]  HasProperty   \n",
       "S0301010413004096-693_T1-2          Quantity    [602, 609]          NaN   \n",
       "S0301010413004096-693_T3-2  MeasuredProperty    [610, 631]  HasQuantity   \n",
       "S0301010413004096-693_T4-2    MeasuredEntity    [502, 504]  HasProperty   \n",
       "S0301010413004096-693_T5-2         Qualifier    [431, 469]    Qualifies   \n",
       "S0301010413004096-693_T1-3          Quantity  [1174, 1184]          NaN   \n",
       "S0301010413004096-693_T3-3    MeasuredEntity  [1130, 1170]  HasQuantity   \n",
       "\n",
       "                           linkId      linkSpan       subSpan unit  \\\n",
       "comboId                                                              \n",
       "S0301010413004096-693_T1-1    NaN           NaN           NaN    K   \n",
       "S0301010413004096-693_T3-1   T1-1    [575, 579]    [548, 579]  NaN   \n",
       "S0301010413004096-693_T4-1   T3-1    [548, 560]    [538, 560]  NaN   \n",
       "S0301010413004096-693_T1-2    NaN           NaN           NaN  wt%   \n",
       "S0301010413004096-693_T3-2   T1-2    [602, 609]    [602, 631]  NaN   \n",
       "S0301010413004096-693_T4-2   T3-2    [610, 631]    [502, 631]  NaN   \n",
       "S0301010413004096-693_T5-2   T3-2    [610, 631]    [431, 631]  NaN   \n",
       "S0301010413004096-693_T1-3    NaN           NaN           NaN    K   \n",
       "S0301010413004096-693_T3-3   T1-3  [1174, 1184]  [1130, 1184]  NaN   \n",
       "\n",
       "                             unitEncoded misc  \n",
       "comboId                                        \n",
       "S0301010413004096-693_T1-1         [530]  NaN  \n",
       "S0301010413004096-693_T3-1           NaN  NaN  \n",
       "S0301010413004096-693_T4-1           NaN  NaN  \n",
       "S0301010413004096-693_T1-2  [43192, 207]  NaN  \n",
       "S0301010413004096-693_T3-2           NaN  NaN  \n",
       "S0301010413004096-693_T4-2           NaN  NaN  \n",
       "S0301010413004096-693_T5-2           NaN  NaN  \n",
       "S0301010413004096-693_T1-3         [530]  NaN  \n",
       "S0301010413004096-693_T3-3           NaN  NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_annot_processed.loc[combo_annot['docId']==demo_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Several  H  cry  temperatures ,  77 )  ∼  total  In  below \n"
     ]
    }
   ],
   "source": [
    "print(labeled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 0, '</s>': 2, '<unk>': 3, '<pad>': 1, '<mask>': 50264}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_token_map = dict(zip(tokenizer.all_special_tokens,tokenizer.all_special_ids))\n",
    "special_token_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0301010413004096-693_T3-1\n",
      "MeasuredProperty\n",
      "HasQuantity\n",
      "548 579\n",
      "[3971, 6, 3700, 23, 6791, 229]\n",
      "[2, 2, 0, 0, 1, 0]\n",
      " temperatures, typically at 77 K\n",
      "\n",
      "S0301010413004096-693_T4-1\n",
      "MeasuredEntity\n",
      "HasProperty\n",
      "538 560\n",
      "[8930, 23982, 3971]\n",
      "[3, 0, 2]\n",
      " cryogenic temperatures\n",
      "\n",
      "S0301010413004096-693_T3-2\n",
      "MeasuredProperty\n",
      "HasQuantity\n",
      "602 631\n",
      "[49447, 1549, 885, 90, 207, 746, 33646, 2148]\n",
      "[1, 0, 0, 0, 0, 2, 0, 0]\n",
      " ∼16 wt% total uptake capacity\n",
      "\n",
      "S0301010413004096-693_T4-2\n",
      "MeasuredEntity\n",
      "HasProperty\n",
      "502 631\n",
      "[289, 176, 5814, 368, 24802, 23549, 36, 41324, 23, 8930, 23982, 3971, 6, 3700, 23, 6791, 229, 43, 646, 306, 742, 19, 10, 638, 9, 49447, 1549, 885, 90, 207, 746, 33646, 2148]\n",
      "[3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0]\n",
      " H2 adsorption capacities (albeit at cryogenic temperatures, typically at 77 K) [4] with a record of ∼16 wt% total uptake capacity\n",
      "\n",
      "S0301010413004096-693_T5-2\n",
      "Qualifier\n",
      "Qualifies\n",
      "431 631\n",
      "[3646, 453, 624, 42, 11757, 597, 284, 33, 4824, 13113, 6608, 239, 289, 176, 5814, 368, 24802, 23549, 36, 41324, 23, 8930, 23982, 3971, 6, 3700, 23, 6791, 229, 43, 646, 306, 742, 19, 10, 638, 9, 49447, 1549, 885, 90, 207, 746, 33646, 2148]\n",
      "[4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0]\n",
      " Several members within this MOF family have achieved impressively high H2 adsorption capacities (albeit at cryogenic temperatures, typically at 77 K) [4] with a record of ∼16 wt% total uptake capacity\n",
      "\n",
      "S0301010413004096-693_T3-3\n",
      "MeasuredEntity\n",
      "HasQuantity\n",
      "1130 1184\n",
      "[96, 20379, 41987, 10477, 25871, 22870, 36, 487, 6153, 43, 23, 874, 158, 229]\n",
      "[3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      " In situ neutron powder diffraction (NPD) at below 10 K\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_annots = combo_annot_processed.loc[combo_annot['docId']==demo_doc]\n",
    "\n",
    "demo_txt = combo_txt[demo_doc]\n",
    "\n",
    "encoded_demo_txt = tokenizer(demo_txt, padding='max_length', max_length=512, truncation=True)\n",
    "demo_token_startchar = []\n",
    "for idx, id in enumerate(encoded_demo_txt['input_ids']):\n",
    "    try: tokenCharStart = encoded_demo_txt.token_to_chars(idx).start\n",
    "    except: tokenCharStart = np.nan\n",
    "    demo_token_startchar.append(tokenCharStart)\n",
    "\n",
    "subSpan_ds = {}\n",
    "for comboId, annot in demo_annots.iterrows():\n",
    "    if isinstance(annot['subSpanType'],float): continue # nans are floats\n",
    "    print(comboId)\n",
    "    print(annot['annotType'])\n",
    "    print(annot['subSpanType'])\n",
    "    print(annot['subSpan'][0],annot['subSpan'][1])\n",
    "    subSpanRange = list(range(annot['subSpan'][0],annot['subSpan'][1]))\n",
    "    # print(subSpanRange)\n",
    "    subSpanIds = []\n",
    "    subSpanLabels = []\n",
    "    for id, label, startChar in zip(demo_ids, demo_labels, demo_token_startchar):\n",
    "        if startChar in subSpanRange:\n",
    "            subSpanIds.append(id)\n",
    "            subSpanLabels.append(label)\n",
    "    print(subSpanIds)\n",
    "    print(subSpanLabels)\n",
    "    print(tokenizer.decode(subSpanIds,skip_special_tokens=True))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tal centres, and that the adsorbed D2 molecules have molecular sepa'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_txt[demo_doc][1391:1458]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Stage1model(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(Stage1model, self).__init__()\n",
    "        self.mod = RobertaModel.from_pretrained(\n",
    "                    model_name,\n",
    "                    num_labels=num_classes+1,\n",
    "                    hidden_dropout_prob=dropout,\n",
    "                    output_hidden_states=True)\n",
    "        self.norm = nn.BatchNorm1d(512, eps=self.mod.config.layer_norm_eps)\n",
    "        self.drop = nn.Dropout(self.mod.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.mod.config.hidden_size, num_classes+1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.mod(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        y_hat = output.hidden_states[-1]\n",
    "        y_hat = self.norm(y_hat)\n",
    "        y_hat = self.drop(y_hat)\n",
    "        y_hat = self.classifier(y_hat).permute(0,2,1)\n",
    "        return y_hat\n",
    "\n",
    "model = Stage1model().to(device)\n",
    "\n",
    "model_new = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stage1model(\n",
       "  (mod): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class OurBERTModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(OurBERTModel, self).__init__()\n",
    "#         self.mod = AutoModel.from_pretrained(model_name, num_labels=num_classes+1)\n",
    "#         self.drop = nn.Dropout(self.mod.config.hidden_dropout_prob)\n",
    "#         self.classifier = nn.Linear(self.mod.config.hidden_size, num_classes+1)\n",
    "\n",
    "#     def forward(self, text, att_mask):\n",
    "#         b, num_tokens = text.shape\n",
    "#         token_type = torch.zeros((b, num_tokens), dtype=torch.long).to(device)\n",
    "#         outputs = self.mod(text, attention_mask=att_mask, token_type_ids=token_type)\n",
    "#         return self.classifier(self.drop(outputs['last_hidden_state']))\n",
    "\n",
    "# model = OurBERTModel().to(device)\n",
    "\n",
    "# model_old = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_logits = model(demo_batch['input_ids'], demo_batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_logits.permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ypred = []\n",
    "# ytrue = []\n",
    "# for dlabels, dlogits in zip(demo_batch['labels'], demo_logits.permute(0,2,1)):\n",
    "#     print(dlabels.shape)\n",
    "#     print(dlogits.shape)\n",
    "#     for tlogits, tlabels in zip(dlogits, dlabels):\n",
    "#         print(tlabels.shape)\n",
    "#         print(tlogits.shape)\n",
    "#         ypred.append(tlogits.argmax().item())\n",
    "#         ytrue.append(tlabels.item())\n",
    "#         print(ypred)\n",
    "#         print(ytrue)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_training_steps = n_epochs * len(batched_train_ds)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=n_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "def train_epoch(ds, criterion):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    ypred = []\n",
    "    ytrue = []\n",
    "    for idx, batch in enumerate(ds):\n",
    "\n",
    "        labels = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        # loss = (loss * attention_mask).sum() / (attention_mask).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "            \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def eval_epoch(ds, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ypred = []\n",
    "    ytrue = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(ds):\n",
    "\n",
    "            labels = batch['labels']\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            # loss = (loss * attention_mask).sum() / (attention_mask).sum()\n",
    "\n",
    "            for dlogits, dlabels in zip(logits.permute(0,2,1), labels):\n",
    "                    for tlogits, tlabels in zip(dlogits, dlabels):\n",
    "                        ypred.append(tlogits.argmax().item())\n",
    "                        ytrue.append(tlabels.item())\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    acc = metrics.accuracy_score(ytrue,ypred)\n",
    "    report = classification_report(ytrue,ypred,\n",
    "                                    labels=list(task_map.values()),\n",
    "                                    target_names=list(task_map.keys()),\n",
    "                                    output_dict=True,\n",
    "                                    zero_division=0)\n",
    "\n",
    "                                    \n",
    "\n",
    "    return loss.item(), acc, report, ytrue, ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba7f487e4aa4b1c8e224760e110f552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Begin Epoch 1 ============\n",
      "Train loss: 72.65139770507812\n",
      "Eval on train set loss: 71.22679138183594   accuracy: 0.977781648089172\n",
      "Eval on dev set loss: 152.74609375   accuracy: 0.9794487847222222\n",
      "============ Begin Epoch 2 ============\n",
      "Train loss: 54.53330993652344\n",
      "Eval on train set loss: 43.23786926269531   accuracy: 0.9803381269904459\n",
      "Eval on dev set loss: 104.6290283203125   accuracy: 0.9807508680555556\n",
      "============ Begin Epoch 3 ============\n",
      "Train loss: 31.65301513671875\n",
      "Eval on train set loss: 31.508602142333984   accuracy: 0.984070212977707\n",
      "Eval on dev set loss: 82.0683364868164   accuracy: 0.9833116319444445\n",
      "============ Begin Epoch 4 ============\n",
      "Train loss: 20.61083221435547\n",
      "Eval on train set loss: 18.85871696472168   accuracy: 0.9878085191082803\n",
      "Eval on dev set loss: 71.9001235961914   accuracy: 0.9852430555555556\n",
      "============ Begin Epoch 5 ============\n",
      "Train loss: 22.31936264038086\n",
      "Eval on train set loss: 19.04261016845703   accuracy: 0.9889841261942676\n",
      "Eval on dev set loss: 77.54388427734375   accuracy: 0.9847439236111111\n",
      "============ Begin Epoch 6 ============\n",
      "Train loss: 17.957975387573242\n",
      "Eval on train set loss: 15.663196563720703   accuracy: 0.9906697850318471\n",
      "Eval on dev set loss: 81.76399230957031   accuracy: 0.9854817708333333\n",
      "============ Begin Epoch 7 ============\n",
      "Train loss: 19.020977020263672\n",
      "Eval on train set loss: 12.395868301391602   accuracy: 0.9917956309713376\n",
      "Eval on dev set loss: 88.15386962890625   accuracy: 0.9858289930555556\n",
      "============ Begin Epoch 8 ============\n",
      "Train loss: 15.675405502319336\n",
      "Eval on train set loss: 12.02721118927002   accuracy: 0.9927099920382165\n",
      "Eval on dev set loss: 83.0035171508789   accuracy: 0.9857421875\n",
      "============ Begin Epoch 9 ============\n",
      "Train loss: 14.485834121704102\n",
      "Eval on train set loss: 15.072978019714355   accuracy: 0.9930583200636943\n",
      "Eval on dev set loss: 87.45355224609375   accuracy: 0.9856770833333334\n",
      "============ Begin Epoch 10 ============\n",
      "Train loss: 11.047452926635742\n",
      "Eval on train set loss: 13.684784889221191   accuracy: 0.9933817675159236\n",
      "Eval on dev set loss: 87.45437622070312   accuracy: 0.9856987847222223\n"
     ]
    }
   ],
   "source": [
    "run_report = {  'epoch':[],\n",
    "                'train_loss':[],\n",
    "                'eval_train_loss':[],\n",
    "                'eval_train_acc':[],\n",
    "                'eval_train_ytrue':[],\n",
    "                'eval_train_ypred':[],\n",
    "                'eval_train_rpt':[],\n",
    "                'eval_dev_loss':[],\n",
    "                'eval_dev_acc':[],\n",
    "                'eval_dev_ytrue':[],\n",
    "                'eval_dev_ypred':[],\n",
    "                'eval_dev_rpt':[],\n",
    "             }\n",
    "\n",
    "num_total_steps = n_epochs * (len(batched_train_ds) * 2 + len(batched_dev_ds))\n",
    "progress_bar = tqdm(range(num_total_steps))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    run_report['epoch'].append(epoch)\n",
    "    \n",
    "    print(f\"============ Begin Epoch {epoch+1} ============\")\n",
    "\n",
    "    loss = train_epoch(batched_train_ds, criterion)\n",
    "    print(f\"Train loss: {loss}\")\n",
    "    run_report['train_loss'].append(loss)\n",
    "    \n",
    "    output = eval_epoch(batched_train_ds, criterion)\n",
    "    (loss, acc, report, ytrue, ypred) = output\n",
    "    print(f'Eval on train set loss: {loss}   accuracy: {acc}')\n",
    "    run_report['eval_train_loss'].append(loss)\n",
    "    run_report['eval_train_acc'].append(acc)\n",
    "    run_report['eval_train_ytrue'].append(ytrue)\n",
    "    run_report['eval_train_ypred'].append(ypred)\n",
    "    run_report['eval_train_rpt'].append(report)\n",
    "\n",
    "    output = eval_epoch(batched_dev_ds, criterion)\n",
    "    (loss, acc, report, ytrue, ypred) = output\n",
    "    print(f'Eval on dev set loss: {loss}   accuracy: {acc}')\n",
    "    run_report['eval_dev_loss'].append(loss)\n",
    "    run_report['eval_dev_acc'].append(acc)\n",
    "    run_report['eval_dev_ytrue'].append(ytrue)\n",
    "    run_report['eval_dev_ypred'].append(ypred)\n",
    "    run_report['eval_dev_rpt'].append(report)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44862    90    78    93    10]\n",
      " [   49   331     1     3     0]\n",
      " [   69     4    88    17     5]\n",
      " [  103     9    43   130     1]\n",
      " [   51     5    14    14    10]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_labels = list(task_map.keys())\n",
    "display_labels.insert(0,str('NoLabel'))\n",
    "cm = confusion_matrix(ytrue,ypred)\n",
    "print(cm)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=display_labels).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ypred)/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_save = ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46080"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ypred_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aligned 512 predictions into doc S0927024813002961-1334\n",
      "aligned 512 predictions into doc S1873506114000075-1132\n",
      "aligned 512 predictions into doc S2211124713006475-841\n",
      "aligned 512 predictions into doc S2211124713006475-1195\n",
      "aligned 512 predictions into doc S0960148113004989-3327\n",
      "aligned 512 predictions into doc S1387700313001822-661\n",
      "aligned 512 predictions into doc S0950705113001895-23137\n",
      "aligned 512 predictions into doc S1873506113001116-1456\n",
      "aligned 512 predictions into doc S0960148113005727-1181\n",
      "aligned 512 predictions into doc S1873506113001116-978\n",
      "aligned 512 predictions into doc S2211124713006475-741\n",
      "aligned 512 predictions into doc S0927024813002961-1051\n",
      "aligned 512 predictions into doc S0927024813003036-1981\n",
      "aligned 512 predictions into doc S1389128612002496-3934\n",
      "aligned 512 predictions into doc S2211124712002884-1110\n",
      "aligned 512 predictions into doc S2211124712002884-649\n",
      "aligned 512 predictions into doc S1750583613004192-1126\n",
      "aligned 512 predictions into doc S0960896612001022-1223\n",
      "aligned 512 predictions into doc S0960148113005727-1451\n",
      "aligned 512 predictions into doc S0960148113004989-2841\n",
      "aligned 512 predictions into doc S1873506114000075-1104\n",
      "aligned 512 predictions into doc S2211124712002884-903\n",
      "aligned 512 predictions into doc S1873506113001116-1369\n",
      "aligned 512 predictions into doc S1386142513006823-2084\n",
      "aligned 512 predictions into doc S0950705113001895-23682\n",
      "aligned 512 predictions into doc S0960148113005727-903\n",
      "aligned 512 predictions into doc S1550413113004920-1550\n",
      "aligned 512 predictions into doc S095741741101342X-726\n",
      "aligned 512 predictions into doc S1570870512000637-1206\n",
      "aligned 512 predictions into doc S1359835X13001875-1359\n",
      "aligned 512 predictions into doc S175058361300203X-1280\n",
      "aligned 512 predictions into doc S0927024813002420-975\n",
      "aligned 512 predictions into doc S0927024813001955-679\n",
      "aligned 512 predictions into doc S0960148113004989-3277\n",
      "aligned 512 predictions into doc S0960148113005727-904\n",
      "aligned 512 predictions into doc S0960148113005727-739\n",
      "aligned 512 predictions into doc S0927024813002961-1085\n",
      "aligned 512 predictions into doc S175058361300203X-1542\n",
      "aligned 512 predictions into doc S0960148113005727-1466\n",
      "aligned 512 predictions into doc S0927775713009606-1216\n",
      "aligned 512 predictions into doc S0960148113005727-1203\n",
      "aligned 512 predictions into doc S175058361300203X-1638\n",
      "aligned 512 predictions into doc S2211124713006475-1205\n",
      "aligned 512 predictions into doc S0960148113002048-3775\n",
      "aligned 512 predictions into doc S1388248113001951-339\n",
      "aligned 512 predictions into doc S2211124712002884-682\n",
      "aligned 512 predictions into doc S0960148113002735-1289\n",
      "aligned 512 predictions into doc S095741741101342X-2624\n",
      "aligned 512 predictions into doc S1873506114000075-1242\n",
      "aligned 512 predictions into doc S0960148113004989-3258\n",
      "aligned 512 predictions into doc S2211124712002884-705\n",
      "aligned 512 predictions into doc S095741741101342X-2642\n",
      "aligned 512 predictions into doc S1750583613004192-1267\n",
      "aligned 512 predictions into doc S1550413113004920-1509\n",
      "aligned 512 predictions into doc S0960148113005727-1494\n",
      "aligned 512 predictions into doc S175058361300203X-1556\n",
      "aligned 512 predictions into doc S2211124712002884-1060\n",
      "aligned 512 predictions into doc S1359645413009816-2973\n",
      "aligned 512 predictions into doc S175058361300203X-1240\n",
      "aligned 512 predictions into doc S0927024813002420-1202\n",
      "aligned 512 predictions into doc S1750583613004192-714\n",
      "aligned 512 predictions into doc S1750583613004192-1689\n",
      "aligned 512 predictions into doc S1161030113001950-923\n",
      "aligned 512 predictions into doc S0950705113001895-23699\n",
      "aligned 512 predictions into doc S1359645413009816-2227\n",
      "aligned 512 predictions into doc S1367912013002277-1213\n",
      "aligned 512 predictions into doc S0927024813002961-1357\n",
      "aligned 512 predictions into doc S1389128612002496-6119\n",
      "aligned 512 predictions into doc S0960148113005727-855\n",
      "aligned 512 predictions into doc S0967064513002774-1376\n",
      "aligned 512 predictions into doc S0960148113002048-3527\n",
      "aligned 512 predictions into doc S0960148113002735-1989\n",
      "aligned 512 predictions into doc S2211124712002884-620\n",
      "aligned 512 predictions into doc S1359645413009816-1712\n",
      "aligned 512 predictions into doc S1084804513001987-7409\n",
      "aligned 512 predictions into doc S0927775713009606-1361\n",
      "aligned 512 predictions into doc S1873506113001116-710\n",
      "aligned 512 predictions into doc S1389128612002496-5994\n",
      "aligned 512 predictions into doc S0927024813001955-812\n",
      "aligned 512 predictions into doc S175058361300203X-1483\n",
      "aligned 512 predictions into doc S1359645413009816-2243\n",
      "aligned 512 predictions into doc S0960148113002735-2182\n",
      "aligned 512 predictions into doc S0927775713009606-1074\n",
      "aligned 512 predictions into doc S0927024813002961-1322\n",
      "aligned 512 predictions into doc S0927024813002420-1032\n",
      "aligned 512 predictions into doc S0927024813003036-2011\n",
      "aligned 512 predictions into doc S1873506114000075-665\n",
      "aligned 512 predictions into doc S0960148113004989-3203\n",
      "aligned 512 predictions into doc S1873506113001116-1204\n",
      "aligned 512 predictions into doc S1389128612002496-6138\n"
     ]
    }
   ],
   "source": [
    "ypred_by_doc = {}\n",
    "ytrue_by_doc = {}\n",
    "for i, doc in enumerate(dev_docs):\n",
    "\n",
    "    ypred_by_doc[doc] = ypred[i*512:(i+1)*512]\n",
    "    ytrue_by_doc[doc] = ytrue[i*512:(i+1)*512]\n",
    "\n",
    "\n",
    "    print(f'aligned {len(ypred_by_doc[doc])} predictions into doc {doc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputpath+'ypred_by_doc.json','w') as f:\n",
    "    json.dump(ypred_by_doc, f)\n",
    "\n",
    "with open(outputpath+'ytrue_by_doc.json','w') as f:\n",
    "    json.dump(ytrue_by_doc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate_metrics(reports):\n",
    "    metrics = []\n",
    "    for epoch in reports:\n",
    "        # print(epoch)\n",
    "        epoch_metrics = {}\n",
    "        for task, rpt in task_map.items():\n",
    "            for metric, value in epoch[task].items():\n",
    "                epoch_metrics[str(task+'_'+metric)] = value\n",
    "        metrics.append(epoch_metrics)\n",
    "\n",
    "    metrics = pd.DataFrame.from_dict(metrics)\n",
    "    metrics.index.rename('epoch')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity_precision</th>\n",
       "      <th>Quantity_recall</th>\n",
       "      <th>Quantity_f1-score</th>\n",
       "      <th>Quantity_support</th>\n",
       "      <th>MeasuredProperty_precision</th>\n",
       "      <th>MeasuredProperty_recall</th>\n",
       "      <th>MeasuredProperty_f1-score</th>\n",
       "      <th>MeasuredProperty_support</th>\n",
       "      <th>MeasuredEntity_precision</th>\n",
       "      <th>MeasuredEntity_recall</th>\n",
       "      <th>MeasuredEntity_f1-score</th>\n",
       "      <th>MeasuredEntity_support</th>\n",
       "      <th>Qualifier_precision</th>\n",
       "      <th>Qualifier_recall</th>\n",
       "      <th>Qualifier_f1-score</th>\n",
       "      <th>Qualifier_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.604799</td>\n",
       "      <td>0.600140</td>\n",
       "      <td>0.602460</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.664523</td>\n",
       "      <td>0.868347</td>\n",
       "      <td>0.752884</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.460055</td>\n",
       "      <td>0.460055</td>\n",
       "      <td>0.460055</td>\n",
       "      <td>726</td>\n",
       "      <td>0.462214</td>\n",
       "      <td>0.487941</td>\n",
       "      <td>0.474729</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.771481</td>\n",
       "      <td>0.886555</td>\n",
       "      <td>0.825024</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.541384</td>\n",
       "      <td>0.549587</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>726</td>\n",
       "      <td>0.604797</td>\n",
       "      <td>0.538033</td>\n",
       "      <td>0.569465</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.812657</td>\n",
       "      <td>0.908263</td>\n",
       "      <td>0.857804</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.556977</td>\n",
       "      <td>0.659780</td>\n",
       "      <td>0.604035</td>\n",
       "      <td>726</td>\n",
       "      <td>0.616682</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.613520</td>\n",
       "      <td>1078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.857513</td>\n",
       "      <td>0.927171</td>\n",
       "      <td>0.890983</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.606768</td>\n",
       "      <td>0.716253</td>\n",
       "      <td>0.656980</td>\n",
       "      <td>726</td>\n",
       "      <td>0.673624</td>\n",
       "      <td>0.658627</td>\n",
       "      <td>0.666041</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.064706</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.852511</td>\n",
       "      <td>0.939076</td>\n",
       "      <td>0.893702</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.646383</td>\n",
       "      <td>0.775482</td>\n",
       "      <td>0.705072</td>\n",
       "      <td>726</td>\n",
       "      <td>0.780432</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.701073</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.091176</td>\n",
       "      <td>0.164894</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.878708</td>\n",
       "      <td>0.933473</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.696187</td>\n",
       "      <td>0.779614</td>\n",
       "      <td>0.735543</td>\n",
       "      <td>726</td>\n",
       "      <td>0.724291</td>\n",
       "      <td>0.757885</td>\n",
       "      <td>0.740707</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.179412</td>\n",
       "      <td>0.295400</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.879456</td>\n",
       "      <td>0.950280</td>\n",
       "      <td>0.913497</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.707407</td>\n",
       "      <td>0.789256</td>\n",
       "      <td>0.746094</td>\n",
       "      <td>726</td>\n",
       "      <td>0.723842</td>\n",
       "      <td>0.782931</td>\n",
       "      <td>0.752228</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>0.229412</td>\n",
       "      <td>0.358621</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.899801</td>\n",
       "      <td>0.949580</td>\n",
       "      <td>0.924020</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.721106</td>\n",
       "      <td>0.790634</td>\n",
       "      <td>0.754271</td>\n",
       "      <td>726</td>\n",
       "      <td>0.736568</td>\n",
       "      <td>0.788497</td>\n",
       "      <td>0.761649</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.252941</td>\n",
       "      <td>0.383073</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quantity_precision  Quantity_recall  Quantity_f1-score  Quantity_support  \\\n",
       "0            0.000000         0.000000           0.000000              1428   \n",
       "1            0.604799         0.600140           0.602460              1428   \n",
       "2            0.664523         0.868347           0.752884              1428   \n",
       "3            0.771481         0.886555           0.825024              1428   \n",
       "4            0.812657         0.908263           0.857804              1428   \n",
       "5            0.857513         0.927171           0.890983              1428   \n",
       "6            0.852511         0.939076           0.893702              1428   \n",
       "7            0.878708         0.933473           0.905263              1428   \n",
       "8            0.879456         0.950280           0.913497              1428   \n",
       "9            0.899801         0.949580           0.924020              1428   \n",
       "\n",
       "   MeasuredProperty_precision  MeasuredProperty_recall  \\\n",
       "0                    0.000000                 0.000000   \n",
       "1                    0.000000                 0.000000   \n",
       "2                    0.460055                 0.460055   \n",
       "3                    0.541384                 0.549587   \n",
       "4                    0.556977                 0.659780   \n",
       "5                    0.606768                 0.716253   \n",
       "6                    0.646383                 0.775482   \n",
       "7                    0.696187                 0.779614   \n",
       "8                    0.707407                 0.789256   \n",
       "9                    0.721106                 0.790634   \n",
       "\n",
       "   MeasuredProperty_f1-score  MeasuredProperty_support  \\\n",
       "0                   0.000000                       726   \n",
       "1                   0.000000                       726   \n",
       "2                   0.460055                       726   \n",
       "3                   0.545455                       726   \n",
       "4                   0.604035                       726   \n",
       "5                   0.656980                       726   \n",
       "6                   0.705072                       726   \n",
       "7                   0.735543                       726   \n",
       "8                   0.746094                       726   \n",
       "9                   0.754271                       726   \n",
       "\n",
       "   MeasuredEntity_precision  MeasuredEntity_recall  MeasuredEntity_f1-score  \\\n",
       "0                  0.000000               0.000000                 0.000000   \n",
       "1                  0.000000               0.000000                 0.000000   \n",
       "2                  0.462214               0.487941                 0.474729   \n",
       "3                  0.604797               0.538033                 0.569465   \n",
       "4                  0.616682               0.610390                 0.613520   \n",
       "5                  0.673624               0.658627                 0.666041   \n",
       "6                  0.780432               0.636364                 0.701073   \n",
       "7                  0.724291               0.757885                 0.740707   \n",
       "8                  0.723842               0.782931                 0.752228   \n",
       "9                  0.736568               0.788497                 0.761649   \n",
       "\n",
       "   MeasuredEntity_support  Qualifier_precision  Qualifier_recall  \\\n",
       "0                    1078             0.000000          0.000000   \n",
       "1                    1078             0.000000          0.000000   \n",
       "2                    1078             0.000000          0.000000   \n",
       "3                    1078             0.000000          0.000000   \n",
       "4                    1078             1.000000          0.011765   \n",
       "5                    1078             0.956522          0.064706   \n",
       "6                    1078             0.861111          0.091176   \n",
       "7                    1078             0.835616          0.179412   \n",
       "8                    1078             0.821053          0.229412   \n",
       "9                    1078             0.788991          0.252941   \n",
       "\n",
       "   Qualifier_f1-score  Qualifier_support  \n",
       "0            0.000000                340  \n",
       "1            0.000000                340  \n",
       "2            0.000000                340  \n",
       "3            0.000000                340  \n",
       "4            0.023256                340  \n",
       "5            0.121212                340  \n",
       "6            0.164894                340  \n",
       "7            0.295400                340  \n",
       "8            0.358621                340  \n",
       "9            0.383073                340  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_metrics = tabulate_metrics(run_report['eval_train_rpt'])\n",
    "train_set_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity_precision</th>\n",
       "      <th>Quantity_recall</th>\n",
       "      <th>Quantity_f1-score</th>\n",
       "      <th>Quantity_support</th>\n",
       "      <th>MeasuredProperty_precision</th>\n",
       "      <th>MeasuredProperty_recall</th>\n",
       "      <th>MeasuredProperty_f1-score</th>\n",
       "      <th>MeasuredProperty_support</th>\n",
       "      <th>MeasuredEntity_precision</th>\n",
       "      <th>MeasuredEntity_recall</th>\n",
       "      <th>MeasuredEntity_f1-score</th>\n",
       "      <th>MeasuredEntity_support</th>\n",
       "      <th>Qualifier_precision</th>\n",
       "      <th>Qualifier_recall</th>\n",
       "      <th>Qualifier_f1-score</th>\n",
       "      <th>Qualifier_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.548209</td>\n",
       "      <td>0.518229</td>\n",
       "      <td>0.532798</td>\n",
       "      <td>384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.625243</td>\n",
       "      <td>0.838542</td>\n",
       "      <td>0.716352</td>\n",
       "      <td>384</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>183</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.391608</td>\n",
       "      <td>0.413284</td>\n",
       "      <td>286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.699561</td>\n",
       "      <td>0.830729</td>\n",
       "      <td>0.759524</td>\n",
       "      <td>384</td>\n",
       "      <td>0.376238</td>\n",
       "      <td>0.415301</td>\n",
       "      <td>0.394805</td>\n",
       "      <td>183</td>\n",
       "      <td>0.541872</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.449898</td>\n",
       "      <td>286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.726437</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.771673</td>\n",
       "      <td>384</td>\n",
       "      <td>0.357447</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>0.401914</td>\n",
       "      <td>183</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.748268</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.793146</td>\n",
       "      <td>384</td>\n",
       "      <td>0.376569</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.426540</td>\n",
       "      <td>183</td>\n",
       "      <td>0.525114</td>\n",
       "      <td>0.402098</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.735892</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.788392</td>\n",
       "      <td>384</td>\n",
       "      <td>0.394068</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.443914</td>\n",
       "      <td>183</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.335664</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.841146</td>\n",
       "      <td>0.798517</td>\n",
       "      <td>384</td>\n",
       "      <td>0.393013</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.436893</td>\n",
       "      <td>183</td>\n",
       "      <td>0.526971</td>\n",
       "      <td>0.444056</td>\n",
       "      <td>0.481973</td>\n",
       "      <td>286</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.745011</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.804790</td>\n",
       "      <td>384</td>\n",
       "      <td>0.387665</td>\n",
       "      <td>0.480874</td>\n",
       "      <td>0.429268</td>\n",
       "      <td>183</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.465035</td>\n",
       "      <td>0.486289</td>\n",
       "      <td>286</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.753986</td>\n",
       "      <td>0.861979</td>\n",
       "      <td>0.804374</td>\n",
       "      <td>384</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.480874</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>183</td>\n",
       "      <td>0.505837</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.478821</td>\n",
       "      <td>286</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quantity_precision  Quantity_recall  Quantity_f1-score  Quantity_support  \\\n",
       "0            0.000000         0.000000           0.000000               384   \n",
       "1            0.548209         0.518229           0.532798               384   \n",
       "2            0.625243         0.838542           0.716352               384   \n",
       "3            0.699561         0.830729           0.759524               384   \n",
       "4            0.726437         0.822917           0.771673               384   \n",
       "5            0.748268         0.843750           0.793146               384   \n",
       "6            0.735892         0.848958           0.788392               384   \n",
       "7            0.760000         0.841146           0.798517               384   \n",
       "8            0.745011         0.875000           0.804790               384   \n",
       "9            0.753986         0.861979           0.804374               384   \n",
       "\n",
       "   MeasuredProperty_precision  MeasuredProperty_recall  \\\n",
       "0                    0.000000                 0.000000   \n",
       "1                    0.000000                 0.000000   \n",
       "2                    0.328358                 0.360656   \n",
       "3                    0.376238                 0.415301   \n",
       "4                    0.357447                 0.459016   \n",
       "5                    0.376569                 0.491803   \n",
       "6                    0.394068                 0.508197   \n",
       "7                    0.393013                 0.491803   \n",
       "8                    0.387665                 0.480874   \n",
       "9                    0.392857                 0.480874   \n",
       "\n",
       "   MeasuredProperty_f1-score  MeasuredProperty_support  \\\n",
       "0                   0.000000                       183   \n",
       "1                   0.000000                       183   \n",
       "2                   0.343750                       183   \n",
       "3                   0.394805                       183   \n",
       "4                   0.401914                       183   \n",
       "5                   0.426540                       183   \n",
       "6                   0.443914                       183   \n",
       "7                   0.436893                       183   \n",
       "8                   0.429268                       183   \n",
       "9                   0.432432                       183   \n",
       "\n",
       "   MeasuredEntity_precision  MeasuredEntity_recall  MeasuredEntity_f1-score  \\\n",
       "0                  0.000000               0.000000                 0.000000   \n",
       "1                  0.000000               0.000000                 0.000000   \n",
       "2                  0.437500               0.391608                 0.413284   \n",
       "3                  0.541872               0.384615                 0.449898   \n",
       "4                  0.500000               0.409091                 0.450000   \n",
       "5                  0.525114               0.402098                 0.455446   \n",
       "6                  0.607595               0.335664                 0.432432   \n",
       "7                  0.526971               0.444056                 0.481973   \n",
       "8                  0.509579               0.465035                 0.486289   \n",
       "9                  0.505837               0.454545                 0.478821   \n",
       "\n",
       "   MeasuredEntity_support  Qualifier_precision  Qualifier_recall  \\\n",
       "0                     286             0.000000          0.000000   \n",
       "1                     286             0.000000          0.000000   \n",
       "2                     286             0.000000          0.000000   \n",
       "3                     286             0.000000          0.000000   \n",
       "4                     286             0.000000          0.000000   \n",
       "5                     286             0.000000          0.000000   \n",
       "6                     286             0.666667          0.021277   \n",
       "7                     286             0.411765          0.074468   \n",
       "8                     286             0.392857          0.117021   \n",
       "9                     286             0.384615          0.106383   \n",
       "\n",
       "   Qualifier_f1-score  Qualifier_support  \n",
       "0            0.000000                 94  \n",
       "1            0.000000                 94  \n",
       "2            0.000000                 94  \n",
       "3            0.000000                 94  \n",
       "4            0.000000                 94  \n",
       "5            0.000000                 94  \n",
       "6            0.041237                 94  \n",
       "7            0.126126                 94  \n",
       "8            0.180328                 94  \n",
       "9            0.166667                 94  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set_metrics = tabulate_metrics(run_report['eval_dev_rpt'])\n",
    "dev_set_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################## loss plot #######################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(range(len(run_report['epoch'])))\n",
    "y = np.array(run_report['eval_dev_loss'])\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################## accuracy plot #######################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(range(len(run_report['epoch'])))\n",
    "y = np.array(run_report['eval_dev_acc'])\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b5060d9997a95c2acb3a42af5d14caeb5dba3e5b7e20123b9f235f707614ce30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
